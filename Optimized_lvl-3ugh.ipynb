{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import loompy as lp\n",
    "import datetime\n",
    "from itertools import count as iter_count\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "# from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from keras.layers import Dense,  Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.losses import mean_squared_error, mean_absolute_error\n",
    "from keras.activations import relu, elu, linear\n",
    "from keras import backend as K\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, evaluation and test ratios for splitting the dataset \n",
    "TRAIN_RT = 0.70\n",
    "EVAL_RT = 0.15\n",
    "TEST_RT = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Split the dataset in train, test & eval sets (index based split)\n",
    "This function returns indices for the train, test and evaluation sets, given an input Dataset.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "    train_rt: ratio of the train dataset\n",
    "    test_rt:  ratio of the test dataset\n",
    "    eval_rt:  ratio of the evaluation dataset\n",
    "\n",
    "Returns:\n",
    "\n",
    "    train_idx: indices (of the given dataset) for the train dataset\n",
    "    test_idx:  indices (of the given dataset) for the test dataset\n",
    "    evel_idx:  indices (of the given dataset) for the evaluation dataset\n",
    "\n",
    "Note:\n",
    "\n",
    "    This function will work correctly as long as (test_rt == evel_rt) is True.\n",
    "    If you need (test_rt != evel_rt), you need something more sophisticated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_set_idx_split(train_rt, test_rt, eval_rt):\n",
    "    with lp.connect('/proj/notebooks/data_QC_min250_GpC_f.loom', 'r') as ds:\n",
    "        idx = np.array(range(0, ds.shape[1]))\n",
    "\n",
    "    train_idx, test_idx = train_test_split(idx, train_size=train_rt, test_size=test_rt+eval_rt, random_state=0)\n",
    "    test_idx, eval_idx = train_test_split(test_idx, train_size=0.5, test_size=0.5, random_state=0)\n",
    "\n",
    "    return train_idx, test_idx, eval_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Coefficient of Determination R2\n",
    "The coefficient of determination R2 can describe how “good” a model is at making predictions. It represents the proportion of the variance in the dependent variable that is predictable from the independent variable,\n",
    "\n",
    "where:\n",
    "\n",
    "    SS_tot: is the total sum of squares,\n",
    "    SS_res: is the residual sum of squares, and is the predicted value.\n",
    "\n",
    "R2 ranges from 0 to 1:\n",
    "\n",
    "    if R2=0 : the model always fails to predict the target variable,\n",
    "    if R2=1 : the model perfectly predicts the target variable.\n",
    "    \n",
    "Any value between 0 and 1 indicates what percentage of the target variable, using the model, can be explained by the features. If R2 < 0, it indicates that the model is no better than one that constantly predicts the mean of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeff_determ_r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res / (SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Data Generator\n",
    "Yields batch features and batch labels.\n",
    "\n",
    "`reset_data()` function, gets indices as an argument, shuffles them and returns them along with an new infinite iterator (starting from 0 with step 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_data(data):\n",
    "    data = shuffle(data)\n",
    "    inf_iterator = iter_count(start = 0, step = 1)\n",
    "    \n",
    "    return inf_iterator, data\n",
    "\n",
    "def generator(batch_size, mode):\n",
    "    with lp.connect('/proj/notebooks/data_QC_min250_GpC_f.loom', 'r') as ds:\n",
    "        \n",
    "        train_idx, test_idx, eval_idx = train_test_set_idx_split(0.7, 0.15, 0.15)\n",
    "        \n",
    "        if mode == 'TRAIN':\n",
    "            indices = train_idx\n",
    "            inf_iterator, indices = reset_data(indices)\n",
    "        elif mode == 'EVAL':\n",
    "            indices = eval_idx\n",
    "            inf_iterator, indices = reset_data(indices)\n",
    "        elif mode == 'TEST':\n",
    "            indices = test_idx\n",
    "            inf_iterator = iter_count(start = 0, step = 1)\n",
    "        else:\n",
    "            print(\"Wrong mode choice: \", mode)\n",
    "            exit(1)\n",
    "\n",
    "        while True:\n",
    "            batch_count = next(inf_iterator)\n",
    "            batch_indices = np.sort(indices[batch_count*batch_size : batch_count*batch_size + batch_size])\n",
    "            \n",
    "            if (mode == 'TRAIN' and batch_count == 1630) or (mode == 'EVAL' and batch_count == 299):\n",
    "                inf_iterator, indices = reset_data(indices)\n",
    "            elif (mode == 'TEST' and batch_count == 69 * 2 - 1):\n",
    "                inf_iterator = iter_count(start = 0, step = 1)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                batch_features = ds[:, batch_indices].T\n",
    "                batch_labels = ds.ca.Age[batch_indices]\n",
    "#                 print(batch_labels, batch_count, batch_indices)\n",
    "            except Exception as e:\n",
    "                print(\"batch_count = {0} - mode: {1} - idx: {2}\".format(batch_count, mode, batch_indices))\n",
    "                print(\"Error: \", e)\n",
    "\n",
    "            yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Test Generator\n",
    "Yields non-shuffled test data and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator():\n",
    "    with lp.connect('/proj/notebooks/data_QC_min250_GpC_f.loom', 'r') as ds:\n",
    "\n",
    "        _, test_idx, _ = train_test_set_idx_split(TRAIN_RT, TEST_RT, EVAL_RT)\n",
    "        idx = np.sort(test_idx)\n",
    "        count = 0\n",
    "      \n",
    "        while True:\n",
    "            # for i in range(batch_size):    ##### Think about total num of cells / batch size\n",
    "            print(count)\n",
    "            count = count + 1\n",
    "            # print(ds.ca.Age[idx])\n",
    "\n",
    "            yield ds[:, idx].T, ds.ca.Age[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Data Function\n",
    "This function provides the train, validation and predict generators for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    train_generator = generator(69, mode = 'TRAIN')\n",
    "    validation_generator = generator(69, mode = 'EVAL')\n",
    "    predict_generator = generator(69, mode = 'TEST')\n",
    "    \n",
    "    return train_generator, validation_generator, predict_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some fixed parameters are based on previous Hyperparameter optimization with Hyperas\n",
    "You may need to change some of the parameters to adapt to your needs/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_generator, validation_generator):\n",
    "\n",
    "    with lp.connect('/proj/notebooks/data_QC_min250_GpC_f.loom', 'r') as ds:\n",
    "        input_size = ds.shape[0]\n",
    "    l1 = regularizers.l1\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dropout({{uniform(0, 1)}}, input_shape=(input_size,)))\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Dense(1000, activation=None))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('elu'))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Dense(1500, activation='relu', activity_regularizer=l1(10e-6)))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(Dense({{choice([500, 200, 100, 50, 10])}}, activation=None))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Layer 4\n",
    "    model.add(Dense({{choice([500, 200, 100, 50, 10])}}, activation='relu', activity_regularizer=l1(10e-6)))\n",
    "\n",
    "    # Last Layer\n",
    "    model.add(Dense(1,  activation={{choice(['linear', 'elu', 'relu', None])}}))\n",
    "    \n",
    "    model.compile(optimizer='nadam', loss='logcosh', metrics=['mse', coeff_determ_r2])\n",
    "    \n",
    "#     earlystopping = EarlyStopping(monitor='val_loss', mode = 'min')\n",
    "#     tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0)\n",
    "#     checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "#     checkpoint = ModelCheckpoint('./mycheckpoints/' + checkpoint_name, monitor='val_loss', verbose = 1, \n",
    "#                              save_best_only = True, mode ='auto')\n",
    "\n",
    "#                         callbacks=[earlystopping, tensorboard, checkpoint],\n",
    "    \n",
    "    hist = model.fit_generator(generator=train_generator,\n",
    "                                epochs = 10,\n",
    "                                steps_per_epoch = 1631,\n",
    "                                validation_data = validation_generator,\n",
    "                                validation_steps= 300,\n",
    "                                workers=1,\n",
    "                                use_multiprocessing=False)\n",
    "        \n",
    "    metrics = model.evaluate_generator(generator=validation_generator, steps= 100)\n",
    "    \n",
    "    return {'loss': -metrics[2], 'mse': metrics[1], 'l': metrics[0], 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions to be used by the model\n",
    "functions=[generator, coeff_determ_r2, reset_data, train_test_set_idx_split]\n",
    "\n",
    "# Get the optimization results\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      functions=functions,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Optimized_lvl-3ugh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best run parameter\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Validation\n",
    "\n",
    "_, validation_generator, _ = data()\n",
    "print(\"Evaluation of best performing model:\")\n",
    "metrics = best_model.evaluate_generator(generator=validation_generator, \n",
    "                                      steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print validation result metrics \n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('best_model-hpr_v003.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_weights('best_model_weights-hpr_v003.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data and labels\n",
    "_, test_idx, _ = train_test_set_idx_split(TRAIN_RT, TEST_RT, EVAL_RT)\n",
    "\n",
    "with lp.connect('/proj/notebooks/data_QC_min250_GpC_f.loom', 'r') as ds:\n",
    "    age = ds.ca.Age[np.sort(test_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model if needed\n",
    "#new_model = load_model('best_model-hpr_v003.h5', custom_objects={'coeff_determ_r2': coeff_determ_r2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test ages\n",
    "test_gen = test_generator()\n",
    "best_model.predict_generator(test_gen, steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram plot of predicted age vs real age for each cell in the test data\n",
    "%matplotlib notebook\n",
    "plt.hist(predict, color='r', bins='fd', label='Predicted Age')\n",
    "plt.hist(age, color='g', bins='fd', alpha=0.5, label='Original Age')\n",
    "plt.legend()\n",
    "plt.xlabel('Age', fontsize=16)\n",
    "plt.ylabel('# of cells', fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 TF-GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
